#!/usr/bin/env python3
"""
Content Factory - Production-grade Content Workflow System

Features:
- Topic discovery via Tavily API + GitHub API
- Multi-AI provider support (Yunwu, OpenRouter, Groq, DeepSeek, Silicon)
- Content generation with templates
- Multi-platform publishing workflow
- Cron-ready automation

API Providers (æŒ‰æ€§ä»·æ¯”æ’åº):
1. Groq - æœ€å¿«æœ€ä¾¿å®œ ($0.0003/1k)
2. DeepSeek - ä¾¿å®œ ($0.00014/1k)
3. SiliconFlow - ä¾¿å®œ ($0.0001/1k)
4. OpenRouter - å¤šæ¨¡å‹ ($0.003/1k Claude)
5. Yunwu - Claude å›½å†… ($0.003/1k)
"""

import os
import sys
import json
import yaml
import time
import hashlib
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
import requests

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/workflow.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class Platform(Enum):
    """Publishing platforms"""
    ZHIHU = "zhihu"
    BILIBILI = "bilibili"
    XIAOHONGSHU = "xiaohongshu"
    GITHUB = "github"
    BLOG = "blog"
    NOTION = "notion"


class ContentStatus(Enum):
    """Content status"""
    DRAFT = "draft"
    REVIEW = "review"
    SCHEDULED = "scheduled"
    PUBLISHED = "published"
    ARCHIVED = "archived"


@dataclass
class Topic:
    """Discovered topic"""
    id: str
    title: str
    description: str
    keywords: List[str]
    source: str
    url: str
    engagement: int
    discovered_at: str
    used: bool = False
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class Content:
    """Content item"""
    id: str
    topic_id: str
    platform: Platform
    title: str
    body: str
    outline: str
    seo_description: str
    tags: List[str]
    status: ContentStatus
    created_at: str
    updated_at: str
    published_at: Optional[str] = None
    views: int = 0
    likes: int = 0
    comments: int = 0
    
    def to_dict(self) -> Dict:
        data = asdict(self)
        data['platform'] = self.platform.value
        data['status'] = self.status.value
        return data


class TavilyClient:
    """Tavily API client for topic research"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.tavily.com"
    
    def search(self, query: str, topic: str = "general",
               days: int = 7, max_results: int = 10,
               include_raw_content: bool = False) -> List[Dict]:
        """Search for topics"""
        try:
            response = requests.post(
                f"{self.base_url}/search",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "query": query,
                    "topic": topic,
                    "days": days,
                    "max_results": max_results,
                    "include_answer": True,
                    "include_raw_content": include_raw_content,
                },
                timeout=30
            )
            response.raise_for_status()
            return response.json().get("results", [])
        except Exception as e:
            logger.error(f"Tavily search error: {e}")
            return []
    
    def get_answer(self, query: str) -> str:
        """Get direct answer"""
        try:
            response = requests.post(
                f"{self.base_url}/answer",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={"query": query},
                timeout=30
            )
            response.raise_for_status()
            return response.json().get("answer", "")
        except Exception as e:
            logger.error(f"Tavily answer error: {e}")
            return ""


class GitHubClient:
    """GitHub API client for trending topics"""
    
    def __init__(self):
        self.base_url = "https://api.github.com"
        self.headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "ContentFactory/1.0"
        }
    
    def get_trending(self, language: str = "python", 
                     since: str = "daily") -> List[Dict]:
        """Get trending repositories"""
        try:
            query = f"language:{language} stars:>1000"
            if since == "daily":
                yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
                query += f" created:>{yesterday}"
            
            response = requests.get(
                f"{self.base_url}/search/repositories",
                headers=self.headers,
                params={"q": query, "sort": "stars", "order": "desc", "per_page": 10},
                timeout=30
            )
            response.raise_for_status()
            items = response.json().get("items", [])
            
            return [{
                "title": f"{r['owner']['login']}/{r['name']}",
                "description": r.get("description", ""),
                "stars": r["stargazers_count"],
                "url": r["html_url"],
                "language": r.get("language", ""),
            } for r in items]
        except Exception as e:
            logger.error(f"GitHub API error: {e}")
            return []


class AIClient:
    """
    Multi-AI provider client
    
    Priority (æ€§ä»·æ¯”):
    1. Groq - æœ€å¿«æœ€ä¾¿å®œ ($0.0003/1k tokens)
    2. DeepSeek - ä¾¿å®œ ($0.00014/1k tokens)
    3. SiliconFlow - ä¾¿å®œ ($0.0001/1k tokens)
    4. OpenRouter - Claude ($0.003/1k tokens)
    5. Yunwu - Claude å›½å†… ($0.003/1k tokens)
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.primary_provider = config.get("provider", "groq")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºçš„æä¾›å•†
        self.provider_order = config.get(
            "providers", 
            ["groq", "deepseek", "silicon", "openrouter", "yunwu"]
        )
    
    def generate(self, prompt: str, max_tokens: int = 2000) -> str:
        """Try providers in order until one works"""
        errors = {}
        
        for provider in self.provider_order:
            try:
                method = getattr(self, f"_generate_{provider}", None)
                if method:
                    result = method(prompt, max_tokens)
                    if result and not result.startswith("[AIç”Ÿæˆå†…å®¹]"):
                        return result
                    errors[provider] = "empty/placeholder"
                else:
                    errors[provider] = "no method"
            except Exception as e:
                errors[provider] = str(e)[:30]
        
        # Hard-fail so callers don't accidentally persist a fake/placeholder draft.
        logger.error(f"All providers failed: {errors}")
        raise RuntimeError(f"All AI providers failed: {errors}")
    
    def _generate_groq(self, prompt: str, max_tokens: int) -> str:
        """Groq - æœ€å¿«æœ€ä¾¿å®œ"""
        api_key = os.environ.get("GROQ_API_KEY", "").strip()
        if not api_key:
            raise RuntimeError("Missing GROQ_API_KEY environment variable")
        
        model = self.config.get("model", "llama-3.3-70b-versatile")
        
        response = requests.post(
            "https://api.groq.com/openai/v1/chat/completions",
            headers={"Authorization": f"Bearer {api_key}"},
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7,
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _generate_deepseek(self, prompt: str, max_tokens: int) -> str:
        """DeepSeek - ä¾¿å®œ"""
        api_key = os.environ.get("DEEPSEEK_API_KEY", "").strip()
        if not api_key:
            raise RuntimeError("Missing DEEPSEEK_API_KEY environment variable")
        
        response = requests.post(
            "https://api.deepseek.com/chat/completions",
            headers={"Authorization": f"Bearer {api_key}"},
            json={
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7,
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _generate_silicon(self, prompt: str, max_tokens: int) -> str:
        """SiliconFlow - ä¾¿å®œ"""
        # Support both names; many environments use SILICONFLOW_API_KEY.
        api_key = (os.environ.get("SILICON_API_KEY", "").strip()
                  or os.environ.get("SILICONFLOW_API_KEY", "").strip())
        if not api_key:
            raise RuntimeError("Missing SILICON_API_KEY (or SILICONFLOW_API_KEY) environment variable")
        
        # SiliconFlow model ids differ from DeepSeek/OpenAI-style names.
        # Use a sane default that exists in /v1/models.
        model = (
            self.config.get("silicon_model")
            or self.config.get("siliconflow_model")
            or self.config.get("model")
            or "deepseek-ai/DeepSeek-V3"
        )
        
        response = requests.post(
            "https://api.siliconflow.cn/v1/chat/completions",
            headers={"Authorization": f"Bearer {api_key}"},
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7,
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _generate_openrouter(self, prompt: str, max_tokens: int) -> str:
        """OpenRouter - å¤šæ¨¡å‹"""
        api_key = os.environ.get("OPENROUTER_API_KEY", "").strip()
        if not api_key:
            raise RuntimeError("Missing OPENROUTER_API_KEY environment variable")
        
        model = self.config.get("model", "anthropic/claude-sonnet-4-20250514")
        
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "HTTP-Referer": "https://content-factory.dev",
            },
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7,
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _generate_yunwu(self, prompt: str, max_tokens: int) -> str:
        """Yunwu - Claude å›½å†…"""
        api_key = os.environ.get("YUNWU_API_KEY", "").strip()
        if not api_key:
            raise RuntimeError("Missing YUNWU_API_KEY environment variable")
        
        model = self.config.get("model", "claude-3-5-sonnet")
        
        response = requests.post(
            "https://yunwu.ai/v1/chat/completions",
            headers={"Authorization": f"Bearer {api_key}"},
            json={
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7,
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def generate_article(self, topic: Topic, style: str = "professional", research: str = "", outline: str = "") -> Dict:
        """Generate full article"""
        prompt = f"""
ä½ æ˜¯ä¸­æ–‡å†…å®¹å†™ä½œä¸“å®¶ï¼Œå†™å…¬ä¼—å·å¾ˆå¤šå¹´ï¼Œæ–‡é£åƒçœŸäººï¼šæœ‰ç»å†ã€æœ‰å–èˆã€æœ‰ç»†èŠ‚ï¼Œä¸ç«¯ç€ã€‚ä½ ä¼šæŠŠæŠ€æœ¯/å·¥å…·ç±»ä¿¡æ¯å†™æˆè¯»è€…çœ‹å®Œç«‹åˆ»èƒ½ç”¨çš„æ–‡ç« ã€‚

ã€ç¡¬æ€§è¦æ±‚ã€‘
- è¾“å‡ºè¯­è¨€ï¼šç®€ä½“ä¸­æ–‡
- ç›´æ¥è¾“å‡º Markdown æ­£æ–‡ï¼ˆä¸è¦åŒ… ```markdown ä»£ç å—ï¼‰
- ä¸è¦å‡ºç°â€œä½œä¸ºAI/æˆ‘æ— æ³•/å…è´£å£°æ˜/å‚è€ƒèµ„æ–™â€ç­‰åºŸè¯
- æ–‡ç« è¦æœ‰æ·±åº¦ï¼šè§£é‡Šâ€œä¸ºä»€ä¹ˆâ€ï¼Œç»™å‡ºå–èˆä¸è¾¹ç•Œæ¡ä»¶
- å†™ä½œåƒçœŸäººï¼šå…è®¸å°‘é‡å£è¯­ã€çŸ­å¥ã€æ’å…¥è‡ªå·±çš„åˆ¤æ–­ï¼›é¿å…æ¨¡æ¿å‘³ï¼ˆæ¯”å¦‚â€œæœ¬æ–‡å°†â€¦ä¸‹é¢å°†â€¦â€è¿™ç§ï¼‰
- å†…å®¹å¿…é¡»å…·ä½“ã€å¯æ‰§è¡Œï¼Œé¿å…ç©ºæ³›
- **é»˜è®¤ç¦æ­¢å‡ºç°ä»»ä½•å…·ä½“æ•°å­—ï¼ˆç™¾åˆ†æ¯”/å€æ•°/é‡‘é¢/æ—¶é—´/æ’åç­‰ï¼‰**ã€‚
  - åªæœ‰å½“ã€å¯ç”¨èµ„æ–™ã€‘çš„æ‘˜è¦é‡Œæ˜ç¡®å‡ºç°äº†è¯¥æ•°å­—ï¼Œä½ æ‰å¯ä»¥ä½¿ç”¨ï¼Œå¹¶åœ¨å¥æœ«ç”¨æ‹¬å·æ ‡æ³¨å¯¹åº”æ¥æºé“¾æ¥ï¼ˆURLï¼‰ã€‚
  - å¦‚æœèµ„æ–™é‡Œæ²¡å†™ï¼Œå°±æŠŠè¡¨è¾¾æ”¹æˆä¸å«æ•°å­—çš„ç»éªŒåˆ¤æ–­ï¼ˆä¾‹å¦‚â€œæ˜æ˜¾â€â€œå¤šæ•°â€â€œå°‘æ•°â€â€œå¤§å¹…â€ï¼‰ã€‚

ã€æ–‡ç« ä¿¡æ¯ã€‘
ä¸»é¢˜ï¼š{topic.title}
è¡¥å……æè¿°ï¼š{topic.description}
å…³é”®è¯ï¼š{", ".join(topic.keywords)}
é£æ ¼ï¼š{style}

ã€å¯ç”¨èµ„æ–™ï¼ˆå¿…é¡»ä½¿ç”¨ï¼Œç¦æ­¢å‡­ç©ºç¼–é€ äº‹å®/æ•°æ®ï¼‰ã€‘
{research}

ã€å¤§çº²ï¼ˆå¿…é¡»ä¸¥æ ¼æŒ‰è¿™ä¸ªå†™ï¼Œç»“æ„å¯ä»¥å¾®è°ƒä½†ä¸è¦è·‘é¢˜ï¼‰ã€‘
{outline}

ã€å¼•ç”¨è§„åˆ™ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
- å¼•ç”¨åªèƒ½ç”¨ (S1)/(S2)â€¦ è¿™ç§æ ¼å¼ã€‚
- æ¯ä¸ªäºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰ä¸‹è‡³å°‘å‡ºç° 1 æ¬¡ (Sx)ã€‚
- å¦‚æœä½ å†™ä¸å‡ºæ¥æºï¼Œå°±åˆ æ‰é‚£å¥ç¡¬æ–­è¨€ï¼Œæ”¹æˆç»éªŒåˆ¤æ–­ã€‚

ã€ç»“æ„è¦æ±‚ã€‘
1) æ ‡é¢˜ï¼š1 è¡Œï¼ˆ# å¼€å¤´ï¼‰ï¼Œåƒå…¬ä¼—å·æ ‡é¢˜ï¼šå…·ä½“ã€å¸¦åˆ©ç›Šç‚¹ï¼Œä½†åˆ«æ²¹è…»
2) å¼€å¤´ï¼šç”¨ä¸€ä¸ªçœŸå®åœºæ™¯å¼€å¤´ï¼ˆç¬¬ä¸€äººç§°ï¼‰ï¼Œè®©è¯»è€…â€œçœ‹è§â€é—®é¢˜ï¼›æœ€åæŠ›å‡ºæœ¬æ–‡èƒ½è§£å†³ä»€ä¹ˆ
3) æ­£æ–‡ï¼šè‡³å°‘ 5 ä¸ªäºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰ï¼Œæ¯èŠ‚å¿…é¡»åŒ…å«ï¼š
   - ä½ è‡ªå·±çš„åˆ¤æ–­/åŸåˆ™ï¼ˆ1~2 å¥ï¼‰
   - ä¸€ä¸ªå…·ä½“ä¾‹å­æˆ–å°æ•…äº‹ï¼ˆå¯ä»¥æ˜¯ä½ /èº«è¾¹äºº/é¡¹ç›®ï¼‰
   - ä¸€ä¸ªå¯å¤åˆ¶çš„åšæ³•ï¼ˆæ­¥éª¤/æ¸…å•/æ¨¡æ¿æç¤ºè¯ï¼‰
   - è‡³å°‘ 1 ä¸ªæ¥æºé“¾æ¥ï¼ˆç”¨æ‹¬å·æ”¾ URLï¼‰
4) å¿…é¡»åŒ…å«ä¸€ä¸ªâ€œå¯ç›´æ¥å¤åˆ¶ä½¿ç”¨â€çš„ä¸“åŒºï¼š
   - ## ç›´æ¥æ‹¿å»ç”¨ï¼ˆå¤åˆ¶åŒºï¼‰
   - è‡³å°‘ç»™å‡º 3 ä¸ªæç¤ºè¯æ¨¡æ¿ï¼ˆç”¨ä»£ç å—åŒ…èµ·æ¥ï¼‰
   - ç»™å‡º 1 ä¸ªæµç¨‹æ¸…å•ï¼ˆstep-by-stepï¼‰
5) å¢åŠ ä¸€ä¸ªä¸“é—¨å°èŠ‚ï¼š## æˆ‘è¸©è¿‡çš„å‘ï¼ˆå†™ 3 æ¡ï¼Œè¶Šå…·ä½“è¶Šå¥½ï¼‰
6) ç»“å°¾ï¼šç»™ä¸€ä¸ª 7 å¤©è¡ŒåŠ¨æ¸…å•ï¼ˆå¯æ‰“å‹¾çš„ checklistï¼‰â€”â€”å¿…é¡»å®Œæ•´ 7 æ¡ï¼Œä¸”æ¯æ¡éƒ½è¦å…·ä½“å¯æ‰§è¡Œï¼Œç¦æ­¢ç•™ç©º
7) æ€»å­—æ•°ï¼š1200~1800 å­—

ç°åœ¨å¼€å§‹å†™ã€‚
"""
        content = self.generate(prompt, max_tokens=3000)
        
        # Parse outline
        lines = content.split('\n')
        outline_lines = [l for l in lines if l.startswith('##')]
        outline = '\n'.join(outline_lines)
        
        return {
            "title": topic.title if len(topic.title) < 60 else topic.title[:57] + "...",
            "body": content,
            "outline": outline,
            "seo_description": topic.description[:150],
            "tags": topic.keywords[:5],
        }
    
    def generate_outline(self, topic: Topic, research: str = "") -> str:
        """Generate an outline first to improve depth and structure."""
        prompt = f"""
ä½ æ˜¯èµ„æ·±å…¬ä¼—å·ä½œè€…ã€‚

ç›®æ ‡ï¼šå…ˆè¾“å‡ºæ–‡ç« å¤§çº²ï¼ˆä¸æ˜¯æ­£æ–‡ï¼‰ï¼Œè®©ç»“æ„æ›´åƒçœŸäººæ€è€ƒã€‚

ã€ç¡¬æ€§è¦æ±‚ã€‘
- è¾“å‡ºç®€ä½“ä¸­æ–‡
- åªè¾“å‡ºå¤§çº²ï¼Œä¸è¦å†™æ­£æ–‡æ®µè½
- å¤§çº²å¿…é¡»è¦†ç›–ï¼š
  - å¼€å¤´åœºæ™¯ï¼ˆç¬¬ä¸€äººç§°ï¼‰
  - è‡³å°‘ 5 ä¸ªäºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰
  - ä¸€ä¸ªå›ºå®šç« èŠ‚ï¼š## ç›´æ¥æ‹¿å»ç”¨ï¼ˆå¤åˆ¶åŒºï¼‰ï¼ˆå†™æ¸…æ¥šå°†ç»™å‡ºå“ªäº›æ¨¡æ¿/æ¸…å•ï¼‰
  - ä¸€ä¸ªå›ºå®šç« èŠ‚ï¼š## æˆ‘è¸©è¿‡çš„å‘ï¼ˆåˆ— 3 æ¡è¦ç‚¹ï¼‰
  - ç»“å°¾ï¼š7 å¤©è¡ŒåŠ¨æ¸…å•ï¼ˆåˆ— 7 æ¡è¦ç‚¹ï¼‰
- æ¯ä¸ª ## å°èŠ‚åé¢ç”¨æ‹¬å·æ ‡æ³¨è‡³å°‘ä¸€ä¸ªæ¥æºç¼–å· (Sx)

ã€é€‰é¢˜ã€‘
{topic.title}

ã€å¯ç”¨èµ„æ–™ã€‘
{research}

è¯·ç”¨ Markdown è¾“å‡ºå¤§çº²ï¼ˆåªåŒ…å«æ ‡é¢˜å’Œè¦ç‚¹åˆ—è¡¨ï¼‰ã€‚
"""
        return self.generate(prompt, max_tokens=900)

    def edit_article(self, draft: str, research: str = "") -> str:
        """Second-pass edit: remove template feel, enforce practicality and citations."""
        prompt = f"""
ä½ æ˜¯èµ„æ·±å…¬ä¼—å·ç¼–è¾‘ + ä¸¥æ ¼å®¡ç¨¿äººã€‚

ç›®æ ‡ï¼šæŠŠä¸‹é¢è¿™ç¯‡æ–‡ç« æ”¹åˆ°æ›´åƒçœŸäººã€æ›´æœ‰æ·±åº¦ã€æ›´å¯æ“ä½œï¼Œå¹¶ä¸”ä¸¥æ ¼éµå®ˆå¼•ç”¨è§„åˆ™ã€‚

ã€å¼•ç”¨è§„åˆ™ã€‘
- åªå…è®¸ç”¨ (S1)/(S2)â€¦ æ ¼å¼ã€‚
- æ¯ä¸ª ## å°èŠ‚è‡³å°‘ 1 ä¸ª (Sx)ã€‚
- å‘ç°æ²¡æœ‰æ¥æºæ”¯æ’‘çš„â€œç¡¬æ–­è¨€/æ•°å­—/åŠŸèƒ½æè¿°â€ï¼Œå°±åˆ æ‰æˆ–é™çº§æˆç»éªŒåˆ¤æ–­ã€‚

ã€å†…å®¹è¦æ±‚ã€‘
- æ¯ä¸ª ## å°èŠ‚éƒ½è¦æœ‰ä¸€ä¸ªâ€œå¯å¤åˆ¶åšæ³•â€ï¼ˆæ­¥éª¤/æ¸…å•/æç¤ºè¯ï¼‰ã€‚
- å¿…é¡»åŒ…å«ï¼š## ç›´æ¥æ‹¿å»ç”¨ï¼ˆå¤åˆ¶åŒºï¼‰
  - è‡³å°‘ 3 ä¸ªæç¤ºè¯æ¨¡æ¿ï¼ˆä»£ç å—ï¼‰
  - 1 ä¸ª step-by-step æµç¨‹
- å¿…é¡»åŒ…å«ï¼š## æˆ‘è¸©è¿‡çš„å‘ï¼ˆ3 æ¡ï¼‰
- ç»“å°¾å¿…é¡»æ˜¯ 7 å¤©æ¸…å•ï¼ˆ7 æ¡ï¼Œä¸è®¸å°‘ï¼‰

ã€å¯ç”¨èµ„æ–™ã€‘
{research}

ã€åŸæ–‡ã€‘
{draft}

è¾“å‡ºè¦æ±‚ï¼š
- åªè¾“å‡º Markdown æ­£æ–‡
- ä¸è¦è¾“å‡ºä»»ä½•å‰è¨€/è¯´æ˜
- ä¸è¦ç”¨ ```markdown ä»£ç å—åŒ…è£¹æ­£æ–‡
"""
        return self.generate(prompt, max_tokens=3200)

    def generate_video_script(self, topic: Topic) -> Dict:
        """Generate video script"""
        prompt = f"""
ä½ æ˜¯ä¸­æ–‡è§†é¢‘ç¼–å¯¼ï¼Œç»™ B ç«™åšæŠ€æœ¯ç±»å£æ’­è„šæœ¬ã€‚

ã€ç¡¬æ€§è¦æ±‚ã€‘
- è¾“å‡ºè¯­è¨€ï¼šç®€ä½“ä¸­æ–‡
- ä¸è¦å‡ºç°â€œä½œä¸ºAI/æˆ‘æ— æ³•/å…è´£å£°æ˜â€
- å£è¯­åŒ–ã€èŠ‚å¥å¿«ã€èƒ½ç›´æ¥å¿µ

ã€è§†é¢‘ä¿¡æ¯ã€‘
ä¸»é¢˜ï¼š{topic.title}
è¡¥å……æè¿°ï¼š{topic.description}

ã€ç»“æ„ã€‘
[å¼€åœºç™½ 10ç§’]
[ä¸ºä»€ä¹ˆå€¼å¾—çœ‹ 20ç§’]
[æ­£æ–‡è¦ç‚¹ x4]ï¼ˆæ¯ç‚¹ç»™ä¾‹å­/ç±»æ¯”ï¼‰
[æ€»ç»“]
[äº’åŠ¨å¼•å¯¼]

æ€»æ—¶é•¿ï¼š3-5 åˆ†é’Ÿã€‚

ç°åœ¨å¼€å§‹å†™ï¼š
"""
        content = self.generate(prompt, max_tokens=2000)
        
        return {
            "title": f"ã€AIå®æˆ˜ã€‘{topic.title}",
            "body": content,
            "outline": "å¼€åœºç™½ â†’ è¦ç‚¹1 â†’ è¦ç‚¹2 â†’ è¦ç‚¹3 â†’ ç»“å°¾",
            "seo_description": f"åˆ†äº«å…³äº{topic.title}çš„å®æˆ˜ç»éªŒ",
            "tags": ["AI", "æŠ€æœ¯åˆ†äº«"] + topic.keywords[:3],
        }


class ContentFactory:
    """Main content workflow system"""
    
    def __init__(self, config_file: str = "config.yaml"):
        self.config = self._load_config(config_file)
        self.data_dir = Path(self.config.get("data_dir", "./data"))
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize clients
        self._init_clients()
        
        # Data files
        self.topics_file = self.data_dir / "topics.json"
        self.content_file = self.data_dir / "content.json"
        
        # Initialize files
        if not self.topics_file.exists():
            self.topics_file.write_text("[]")
        if not self.content_file.exists():
            self.content_file.write_text("[]")
        
        logger.info("ğŸš€ ContentFactory initialized")
    
    def _load_config(self, config_file: str) -> Dict:
        """Load configuration"""
        default = {
            "data_dir": "./data",
            "tavily_api_key": os.environ.get("TAVILY_API_KEY", ""),
            # Default to SiliconFlow since this workspace typically has SILICONFLOW_API_KEY configured.
            "ai_provider": "silicon",
            "providers": ["silicon", "deepseek", "groq", "openrouter", "yunwu"],
            # Language control for prompts.
            "output_language": "zh-CN",
            # SiliconFlow model override (must exist in https://api.siliconflow.cn/v1/models)
            "silicon_model": "Pro/moonshotai/Kimi-K2.5",
        }
        
        if Path(config_file).exists():
            with open(config_file) as f:
                user = yaml.safe_load(f)
                default.update(user)
        
        return default
    
    def _init_clients(self):
        """Initialize API clients"""
        # Tavily
        tavily_key = self.config.get("tavily_api_key", "")
        if tavily_key:
            self.tavily = TavilyClient(tavily_key)
            logger.info("âœ… Tavily client ready")
        else:
            self.tavily = None
            logger.info("â„¹ï¸  Tavily not configured (using GitHub only)")
        
        # GitHub
        self.github = GitHubClient()
        logger.info("âœ… GitHub client ready")
        
        # AI
        ai_config = {
            "provider": self.config.get("ai_provider", "groq"),
            "providers": self.config.get("providers", ["groq", "deepseek", "silicon"]),
        }
        self.ai = AIClient(ai_config)
        logger.info(f"âœ… AI client ready ({ai_config['providers'][0]})")
    
    def _gen_id(self, prefix: str) -> str:
        """Generate unique ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{prefix}_{timestamp}"
    
    def discover_topics(self, limit: int = 10) -> List[Topic]:
        """Discover trending topics"""
        topics = []
        
        # GitHub trending
        logger.info("ğŸ“Š Fetching GitHub trending...")
        repos = self.github.get_trending("python", "weekly")
        for r in repos[:5]:
            topic = Topic(
                id=self._gen_id("gh"),
                title=r["title"],
                description=r["description"][:200] if r["description"] else "",
                keywords=[r["language"], "GitHub", "trending"],
                source="github",
                url=r["url"],
                engagement=r["stars"],
                discovered_at=datetime.now().isoformat(),
            )
            topics.append(topic)
        
        # Tavily search
        if self.tavily:
            logger.info("ğŸ” Searching Tavily...")
            queries = [
                "AI automation tools 2026",
                "ChatGPT workflow examples",
                "productivity system automation",
            ]
            for q in queries[:2]:
                results = self.tavily.search(q, max_results=3)
                for i, r in enumerate(results):
                    topic = Topic(
                        id=self._gen_id("tv"),
                        title=r.get("title", q),
                        description=r.get("content", "")[:200],
                        keywords=q.split(),
                        source="tavily",
                        url=r.get("url", ""),
                        engagement=i * 10,
                        discovered_at=datetime.now().isoformat(),
                    )
                    topics.append(topic)
        
        # Save topics
        existing = json.loads(self.topics_file.read_text()) if self.topics_file.exists() else []
        existing.extend([t.to_dict() for t in topics])
        self.topics_file.write_text(json.dumps(existing, indent=2, ensure_ascii=False))
        
        logger.info(f"âœ… Discovered {len(topics)} topics")
        return topics
    
    def get_unused_topics(self, limit: int = 5) -> List[Topic]:
        """Get unused topics"""
        all_topics = json.loads(self.topics_file.read_text()) if self.topics_file.exists() else []
        unused = [Topic(**t) for t in all_topics if not t.get("used", False)]
        return unused[:limit]

    def score_topic(self, t: Topic) -> float:
        """Heuristic scoring for practicality and writeability."""
        s = 0.0
        if t.url:
            s += 2.0
        if t.source == "tavily":
            s += 2.0
        if t.description and len(t.description) >= 80:
            s += 1.0
        # Prefer topics that look like actionable workflows
        kw = (t.title + " " + (t.description or "")).lower()
        for word in ["workflow", "prompt", "template", "automation", "playbook", "checklist", "æ•™ç¨‹", "æ¨¡ç‰ˆ", "æç¤ºè¯", "å·¥ä½œæµ", "è‡ªåŠ¨åŒ–"]:
            if word in kw:
                s += 0.5
        return s

    def choose_best_topic(self, candidates: List[Topic]) -> Optional[Topic]:
        if not candidates:
            return None
        ranked = sorted(candidates, key=self.score_topic, reverse=True)
        return ranked[0]
    
    def mark_topic_used(self, topic_id: str):
        """Mark topic as used"""
        topics = json.loads(self.topics_file.read_text())
        for t in topics:
            if t["id"] == topic_id:
                t["used"] = True
        self.topics_file.write_text(json.dumps(topics, indent=2, ensure_ascii=False))

    def build_research_packet(self, topic: Topic, max_sources: int = 6) -> Tuple[str, List[str]]:
        """Build a compact research packet (numbered sources) for grounded writing.

        Returns: (packet_text, urls)
        """
        lines: List[str] = []
        urls: List[str] = []

        if topic.url:
            urls.append(topic.url)

        if not getattr(self, "tavily", None):
            lines.append("ã€æ¥æºã€‘ï¼ˆæ— ï¼šTavily æœªé…ç½®ï¼‰")
            lines.append("- S1: (no-source)")
            lines.append("ã€è§„åˆ™ã€‘åªèƒ½å†™ç»éªŒåˆ¤æ–­ï¼›ç¦æ­¢ä»»ä½•å…·ä½“æ•°å­—/åŠŸèƒ½æ–­è¨€ã€‚")
            return "\n".join(lines), []

        results = self.tavily.search(
            query=topic.title,
            topic="general",
            days=30,
            max_results=max_sources,
            include_raw_content=False,
        )

        # Prefer Tavily results; then fall back to topic.url if present.
        for r in results[:max_sources]:
            url = (r.get("url") or "").strip()
            if url and url not in urls:
                urls.append(url)

        if topic.url and topic.url not in urls:
            urls.append(topic.url)

        lines.append("ã€æ¥æºï¼ˆå¿…é¡»å¼•ç”¨ï¼Œç”¨ (S1)/(S2) è¿™ç§æ ¼å¼ï¼‰ã€‘")
        for i, url in enumerate(urls, 1):
            lines.append(f"- S{i}: {url}")

        lines.append("\nã€è¦ç‚¹æ‘˜è¦ï¼ˆå†™ä½œå¯ç”¨ï¼Œä½†ç¡¬æ–­è¨€ä»è¦å¼•ç”¨æ¥æºï¼‰ã€‘")
        for r in results[:max_sources]:
            title = (r.get("title") or "").strip()
            url = (r.get("url") or "").strip()
            snippet = (r.get("content") or r.get("answer") or "").strip()
            snippet = " ".join(snippet.split())
            if snippet:
                snippet = snippet[:320]
            if url:
                head = title if title else url
                lines.append(f"- {head}")
                if snippet:
                    lines.append(f"  - {snippet}")

        lines.append("\nã€ç¡¬è§„åˆ™ã€‘")
        lines.append("- é»˜è®¤ç¦æ­¢ç¼–é€ å…·ä½“æ•°å­—/ç»Ÿè®¡/æ”¿ç­–æ³•è§„/äº§å“åŠŸèƒ½æ–­è¨€ã€‚")
        lines.append("- åªè¦å‡ºç°ç¡¬æ–­è¨€ï¼Œå¥æœ«å¿…é¡»æ ‡æ³¨æ¥æºï¼š(Sx)ã€‚")
        return "\n".join(lines), urls

    def _sanitize_markdown(self, text: str) -> str:
        """Strip LLM wrappers like prefaces and fenced ```markdown blocks."""
        if not text:
            return text

        t = text.strip()

        # If the model wrapped the whole article in a markdown fence, extract it.
        if "```markdown" in t:
            start = t.find("```markdown")
            if start != -1:
                start = start + len("```markdown")
                end = t.find("```", start)
                if end != -1:
                    t = t[start:end].strip()

        # Drop any preface before the first markdown title.
        hash_pos = t.find("# ")
        if hash_pos > 0:
            t = t[hash_pos:].lstrip()

        return t

    def generate_content(self, topic: Topic, content_type: str = "article") -> Optional[Content]:
        """Generate content from topic"""
        logger.info(f"ğŸ“ Generating {content_type} for: {topic.title}")
        
        try:
            if content_type == "article":
                research, urls = self.build_research_packet(topic)
                outline = self.ai.generate_outline(topic, research=research)
                result = self.ai.generate_article(topic, research=research, outline=outline)

                body = self._sanitize_markdown(result.get("body", ""))

                # Second-pass edit if citations/copy-zone are missing.
                if ("## ç›´æ¥æ‹¿å»ç”¨" not in body) or ("(S1)" not in body and "(S2)" not in body):
                    body = self._sanitize_markdown(self.ai.edit_article(body, research=research))

                result["body"] = body
            else:
                result = self.ai.generate_video_script(topic)
            
            content = Content(
                id=self._gen_id("c"),
                topic_id=topic.id,
                platform=Platform.ZHIHU,
                title=result["title"],
                body=result["body"],
                outline=result["outline"],
                seo_description=result["seo_description"],
                tags=result["tags"],
                status=ContentStatus.DRAFT,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
            )
            
            # Save content
            existing = json.loads(self.content_file.read_text()) if self.content_file.exists() else []
            existing.append(content.to_dict())
            self.content_file.write_text(json.dumps(existing, indent=2, ensure_ascii=False))
            
            self.mark_topic_used(topic.id)
            logger.info(f"âœ… Generated: {content.id}")
            return content
            
        except Exception as e:
            logger.error(f"Generation error: {e}")
            return None
    
    def get_draft_content(self) -> List[Content]:
        """Get all draft content"""
        if not self.content_file.exists():
            return []
        items = json.loads(self.content_file.read_text())
        return [Content(**c) for c in items if c["status"] == "draft"]
    
    def get_status(self) -> Dict:
        """Get workflow status"""
        topics = json.loads(self.topics_file.read_text()) if self.topics_file.exists() else []
        content = json.loads(self.content_file.read_text()) if self.content_file.exists() else []
        
        return {
            "total_topics": len(topics),
            "unused_topics": len([t for t in topics if not t.get("used", False)]),
            "draft_content": len([c for c in content if c["status"] == "draft"]),
            "published_content": len([c for c in content if c["status"] == "published"]),
        }
    
    def run_daily(self):
        """Run complete daily workflow"""
        logger.info("ğŸš€ Starting daily content workflow...")
        
        # 1. Discover topics
        topics = self.discover_topics()
        
        # 2. Get unused topics
        unused = self.get_unused_topics(10)

        # 3. Pick the most writeable/practical topic
        top_topic = self.choose_best_topic(unused)

        # 4. Generate content for selected topic
        if top_topic:
            content = self.generate_content(top_topic, "article")
            
            if content:
                logger.info(f"ğŸ“„ Generated: {content.title}")
        
        # 4. Summary
        status = self.get_status()
        logger.info(f"\nğŸ“Š Daily Summary:")
        logger.info(f"   Topics: {status['total_topics']} ({status['unused_topics']} unused)")
        logger.info(f"   Drafts: {status['draft_content']}")
        logger.info(f"   Published: {status['published_content']}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Content Factory")
    subparsers = parser.add_subparsers(dest="command")
    
    subparsers.add_parser("daily", help="Run daily workflow")
    subparsers.add_parser("status", help="Show status")
    subparsers.add_parser("discover", help="Discover topics")
    
    gen_parser = subparsers.add_parser("generate", help="Generate content")
    gen_parser.add_argument("topic_id", help="Topic ID")
    gen_parser.add_argument("--type", "-t", default="article", choices=["article", "video"])
    
    args = parser.parse_args()
    
    factory = ContentFactory()
    
    if args.command == "daily":
        factory.run_daily()
    elif args.command == "status":
        s = factory.get_status()
        for k, v in s.items():
            print(f"  {k}: {v}")
    elif args.command == "discover":
        factory.discover_topics()
    elif args.command == "generate":
        topics = factory.get_unused_topics()
        topic = next((t for t in topics if t.id == args.topic_id), None)
        if topic:
            factory.generate_content(topic, args.type)
        else:
            print(f"Topic not found: {args.topic_id}")
