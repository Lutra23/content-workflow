# RAG æŠ€æœ¯æ·±åº¦åˆ†æä¸å®æˆ˜æŒ‡å—

**åˆ†ææ—¥æœŸ**: 2026-01-31
**ä½œè€…**: lutra ğŸ¦¦
**ç›®æ ‡**: æŒæ¡ç”Ÿäº§çº§ RAG ç³»ç»Ÿæ¶æ„

---

## ä¸€ã€RAG æ ¸å¿ƒæ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ RAG

```
RAG = Retrieval Augmented Generation (æ£€ç´¢å¢å¼ºç”Ÿæˆ)

æ ¸å¿ƒæ€æƒ³: å¼€å·è€ƒè¯•
â”œâ”€â”€ è¾“å…¥é—®é¢˜ â†’ æ£€ç´¢çŸ¥è¯† â†’ ç”Ÿæˆç­”æ¡ˆ
â””â”€â”€ ç±»æ¯”: LLM å…ˆç¿»ä¹¦ï¼Œå†å›ç­”
```

### 1.2 RAG vs ä¼ ç»Ÿ LLM

| ç»´åº¦ | çº¯ LLM | RAG |
|------|--------|-----|
| çŸ¥è¯†æ—¶æ•ˆ | è®­ç»ƒæ•°æ®æˆªæ­¢æ—¥æœŸ | å®æ—¶æ›´æ–° |
| ç§æœ‰çŸ¥è¯† | æ— æ³•ç›´æ¥å›ç­” | ç²¾å‡†æ£€ç´¢ |
| å¹»è§‰é—®é¢˜ | å¯èƒ½ç¼–é€  | åŸºäºæ£€ç´¢äº‹å® |
| æˆæœ¬ | ä½ | è¾ƒé«˜ï¼ˆå‘é‡åº“ï¼‰ |

### 1.3 RAG å·¥ä½œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç¦»çº¿æµç¨‹ (Indexing)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ–‡æ¡£åŠ è½½ â†’ æ–‡æœ¬åˆ‡åˆ† â†’ å‘é‡åŒ– â†’ å­˜å…¥å‘é‡æ•°æ®åº“              â”‚
â”‚     â†“            â†“           â†“            â†“                â”‚
â”‚  PDF/TXT      chunk        embedding    Chroma/Pinecone  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åœ¨çº¿æµç¨‹ (Querying)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç”¨æˆ·é—®é¢˜ â†’ é—®é¢˜å‘é‡åŒ– â†’ ç›¸ä¼¼æ£€ç´¢ â†’ æ‹¼ Prompt â†’ LLM ç”Ÿæˆ   â”‚
â”‚      â†“          â†“           â†“            â†“          â†“        â”‚
â”‚  user input  embedding   vector search  context   response â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€å‘é‡æ•°æ®åº“é€‰å‹

### 2.1 ä¸»æµå‘é‡æ•°æ®åº“å¯¹æ¯”

| æ•°æ®åº“ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|----------|
| **Chroma** | è½»é‡ã€æ˜“ç”¨ | åŠŸèƒ½æœ‰é™ | åŸå‹/å°å‹é¡¹ç›® |
| **Pinecone** | æ‰˜ç®¡ã€æ— è¿ç»´ | ä»˜è´¹ | ä¸­å¤§å‹é¡¹ç›® |
| **Weaviate** | å¼€æºã€ç‰¹æ€§å¤š | èµ„æºæ¶ˆè€—å¤§ | å¤æ‚åœºæ™¯ |
| **Milvus** | é«˜æ€§èƒ½ã€å›½äº§ | éƒ¨ç½²å¤æ‚ | è¶…å¤§è§„æ¨¡ |
| **FAISS** | é«˜æ€§èƒ½ã€æœ¬åœ° | æ— æŒä¹…åŒ– | ç¦»çº¿åœºæ™¯ |

### 2.2 Chroma å®æˆ˜

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# åˆå§‹åŒ–
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = Chroma(
    persist_directory="./chromaDB",
    collection_name="demo001",
    embedding_function=embeddings
)

# å­˜å‚¨æ–‡æ¡£
documents = [
    "AI æ˜¯äººå·¥æ™ºèƒ½çš„ç¼©å†™",
    "æœºå™¨å­¦ä¹ æ˜¯ AI çš„å­é¢†åŸŸ",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„æ–¹æ³•"
]
vectorstore.add_texts(documents)

# æ£€ç´¢
results = vectorstore.similarity_search("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", k=3)
```

### 2.3 å‘é‡æ£€ç´¢åŸç†

```
å‘é‡æ£€ç´¢ = ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

Query å‘é‡åŒ– â†’ [0.1, 0.3, 0.8, ...]
                     â†“
              ä¸æ‰€æœ‰æ–‡æ¡£å‘é‡è®¡ç®—ç›¸ä¼¼åº¦
                     â†“
              è¿”å› Top-K æœ€ç›¸ä¼¼æ–‡æ¡£
```

```python
# ç›¸ä¼¼åº¦æ£€ç´¢
results = vectorstore.similarity_search_with_score("ç”¨æˆ·é—®é¢˜", k=5)
for doc, score in results:
    print(f"ç›¸ä¼¼åº¦: {score:.4f}")
    print(f"å†…å®¹: {doc.page_content}")
```

---

## ä¸‰ã€æ–‡æ¡£å¤„ç†æµæ°´çº¿

### 3.1 æ–‡æ¡£åŠ è½½

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader
)

# PDF åŠ è½½
loader = PyPDFLoader("document.pdf")
pages = loader.load()

# æ–‡æœ¬åŠ è½½
loader = TextLoader("document.txt")
docs = loader.load()

# Markdown åŠ è½½
loader = UnstructuredMarkdownLoader("document.md")
docs = loader.load()

# Web é¡µé¢åŠ è½½
from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://example.com")
docs = loader.load()
```

### 3.2 æ–‡æœ¬åˆ‡åˆ†

```python
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter
)

# æ–¹æ³•1: é€’å½’å­—ç¬¦åˆ‡åˆ†
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # æ¯ä¸ª chunk çš„å¤§å°
    chunk_overlap=200,     # é‡å å¤§å°ï¼ˆä¿æŒè¿ç»­æ€§ï¼‰
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " "]  # åˆ†éš”ç¬¦ä¼˜å…ˆçº§
)

chunks = text_splitter.split_documents(docs)

# æ–¹æ³•2: Markdown æ ‡é¢˜åˆ‡åˆ†
headers_to_split_on = [
    ("#", "H1"),
    ("##", "H2"),
    ("###", "H3")
]
splitter = MarkdownHeaderTextSplitter(headers_to_split_on)
chunks = splitter.split_text(doc_text)
```

### 3.3 åˆ‡åˆ†ç­–ç•¥å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åˆ‡åˆ†ç­–ç•¥å¯¹æ¯”                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   å›ºå®šé•¿åº¦   â”‚ ç®€å•ï¼Œä½†å¯èƒ½åˆ‡æ–­è¯­ä¹‰                        â”‚
â”‚   é€’å½’åˆ‡åˆ†   â”‚ æŒ‰æ®µè½/å¥å­åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§ â­æ¨è      â”‚
â”‚   è¯­ä¹‰åˆ‡åˆ†   â”‚ æŒ‰ä¸»é¢˜åˆ‡åˆ†ï¼Œä½†å®ç°å¤æ‚                    â”‚
â”‚   æ ‡é¢˜åˆ‡åˆ†   â”‚ é€‚åˆ Markdown ç»“æ„åŒ–æ–‡æ¡£                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å››ã€Embedding æ¨¡å‹é€‰å‹

### 4.1 ä¸»æµ Embedding æ¨¡å‹

| æ¨¡å‹ | ç»´åº¦ | æ•ˆæœ | æˆæœ¬ | é€Ÿåº¦ |
|------|------|------|------|------|
| text-embedding-3-small | 1536 | â­â­â­ | ä½ | å¿« |
| text-embedding-3-large | 3072 | â­â­â­â­â­ | ä¸­ | ä¸­ |
| BGE-large-zh | 1024 | â­â­â­â­ | ä½ | å¿« |
| M3E-large | 1024 | â­â­â­â­ | ä½ | å¿« |

### 4.2 å¤šæ¨¡å‹æ”¯æŒ

```python
from langchain_openai import OpenAIEmbeddings

# OpenAI
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# é˜¿é‡Œé€šä¹‰
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="text-embedding-v1"
)

# æœ¬åœ° Ollama
from langchain_community.embeddings import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="nomic-embed-text:latest"
)
```

---

## äº”ã€Prompt Engineering

### 5.1 RAG Prompt æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# åŸºç¡€ RAG Prompt
PROMPT_TEMPLATE = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚
"""

# å¸¦æ€è€ƒçš„ RAG Prompt
PROMPT_WITH_THINKING = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å›ç­”ï¼š

1. åˆ†æé—®é¢˜ï¼Œç†è§£ç”¨æˆ·æ„å›¾
2. åœ¨ä¸Šä¸‹æ–‡ä¸­æœç´¢ç›¸å…³ä¿¡æ¯
3. ç»¼åˆåˆ†æï¼Œç»™å‡ºç­”æ¡ˆ

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

è¯·é€æ­¥æ€è€ƒå¹¶ç»™å‡ºç­”æ¡ˆã€‚
"""
```

### 5.2 LCEL æ„å»º Chain

```python
from langchain_core.runnables import RunnablePassthrough

# æ£€ç´¢å™¨
retriever = vectorstore.as_retriever(k=5)

# Prompt æ¨¡æ¿
prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

# LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# æ„å»º Chain (LCEL)
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

# æ‰§è¡Œ
response = chain.invoke("ç”¨æˆ·é—®é¢˜")
```

---

## å…­ã€æ£€ç´¢ä¼˜åŒ–ç­–ç•¥

### 6.1 æŸ¥è¯¢å˜æ¢

```python
# æŸ¥è¯¢é‡å†™ - æ‰©å±•æŸ¥è¯¢è¯
def expand_query(question: str) -> list[str]:
    """å°†é—®é¢˜æ‰©å±•ä¸ºå¤šä¸ªæŸ¥è¯¢"""
    return [
        question,
        f"ä»€ä¹ˆæ˜¯{question}",
        f"{question}çš„åŸç†",
        f"å…³äº{question}çš„çŸ¥è¯†"
    ]

# æŸ¥è¯¢åˆ†è§£ - æ‹†åˆ†ä¸ºå­é—®é¢˜
def decompose_query(question: str) -> list[str]:
    """å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºç®€å•é—®é¢˜"""
    # ä½¿ç”¨ LLM è¿›è¡Œåˆ†è§£
    prompt = f"å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªç®€å•é—®é¢˜:\n{question}"
    sub_questions = llm.invoke(prompt)
    return sub_questions.split("\n")
```

### 6.2 Re-ranker é‡æ’åº

```python
from langchain_community.cross_encoders import CrossEncoder

# è½»é‡çº§é‡æ’åºæ¨¡å‹
reranker = CrossEncoder("BAAI/bge-reranker-base")

# ä¸¤é˜¶æ®µæ£€ç´¢
def retrieve_with_rerank(query: str, k1: int = 20, k2: int = 5):
    # é˜¶æ®µ1: ç²—æ£€ç´¢
    initial_results = vectorstore.similarity_search(query, k=k1)
    
    # é˜¶æ®µ2: é‡æ’åº
    pairs = [(query, doc.page_content) for doc in initial_results]
    scores = reranker.predict(pairs)
    
    # æ’åºå¹¶è¿”å› Top-K
    ranked_docs = sorted(
        zip(initial_results, scores),
        key=lambda x: x[1],
        reverse=True
    )[:k2]
    
    return [doc for doc, _ in ranked_docs]
```

### 6.3 æ··åˆæ£€ç´¢

```python
# å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢
from langchain_community.retrievers import BM25Retriever

# å‘é‡æ£€ç´¢
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# å…³é”®è¯æ£€ç´¢
texts = [doc.page_content for doc in vectorstore.similarity_search("", k=100)]
keyword_retriever = BM25Retriever.from_texts(texts)

# èåˆæ£€ç´¢ç»“æœ
from langchain.retrievers import EnsembleRetriever

ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, keyword_retriever],
    weights=[0.7, 0.3]  # å‘é‡æ£€ç´¢æƒé‡æ›´é«˜
)
```

---

## ä¸ƒã€è®°å¿†ç³»ç»Ÿé›†æˆ

### 7.1 å¯¹è¯å†å²ç®¡ç†

```python
# çŸ­æœŸè®°å¿† - çª—å£æ»‘åŠ¨
def get_recent_messages(messages: list, window: int = 5):
    """ä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯"""
    return messages[-window:]

# é•¿æœŸè®°å¿† - å‘é‡å­˜å‚¨
def store_memory(question: str, answer: str, user_id: str):
    """å­˜å‚¨é—®ç­”å¯¹åˆ°é•¿æœŸè®°å¿†"""
    memory_text = f"é—®é¢˜: {question}\nç­”æ¡ˆ: {answer}"
    vectorstore.add_texts([memory_text], metadatas=[{"user_id": user_id}])

# æ£€ç´¢ç›¸å…³å†å²
def get_relevant_history(query: str, user_id: str):
    """æ£€ç´¢ç”¨æˆ·ç›¸å…³å†å²"""
    results = vectorstore.similarity_search(
        query,
        k=3,
        filter={"user_id": user_id}
    )
    return [r.page_content for r in results]
```

### 7.2 å®Œæ•´è®°å¿†æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è®°å¿†å¤„ç†æµæ°´çº¿                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  ç”¨æˆ·è¾“å…¥ â†’ æ£€ç´¢å†å² â†’ æ‹¼å…¥ä¸Šä¸‹æ–‡ â†’ LLM ç”Ÿæˆ          â”‚
â”‚      â†“           â†“            â†“            â†“          â”‚
â”‚  æ–°é—®é¢˜   çŸ­æœŸè®°å¿†     å¢å¼º Prompt    ç”Ÿæˆå›ç­”       â”‚
â”‚            â†“                               â†“          â”‚
â”‚         é•¿æœŸè®°å¿† â†â”€â”€ å­˜å‚¨æ–°é—®ç­” â”€â”€â†’ å†å²è®°å½•        â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å…«ã€å®Œæ•´ä»£ç æ¶æ„

### 8.1 é¡¹ç›®ç»“æ„

```
RagLangChainTest/
â”œâ”€â”€ main.py                 # FastAPI æœåŠ¡å…¥å£
â”œâ”€â”€ apiTest.py             # API æµ‹è¯•è„šæœ¬
â”œâ”€â”€ mainMemory.py          # å¸¦è®°å¿†çš„ç‰ˆæœ¬
â”œâ”€â”€ mainReranker.py        # å¸¦é‡æ’åºçš„ç‰ˆæœ¬
â”œâ”€â”€ prompt_template.txt    # Prompt æ¨¡æ¿
â”œâ”€â”€ prompt_template_memory.txt  # å¸¦è®°å¿†çš„ Prompt
â”œâ”€â”€ chromaDB/              # å‘é‡æ•°æ®åº“å­˜å‚¨
â”œâ”€â”€ input/                 # è¾“å…¥æ–‡æ¡£
â”œâ”€â”€ output/                # è¾“å‡ºæ–‡ä»¶
â””â”€â”€ tools/                 # å·¥å…·å‡½æ•°
```

### 8.2 FastAPI æœåŠ¡

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List

app = FastAPI(title="RAG API")

class QueryRequest(BaseModel):
    question: str
    user_id: str = "default"

class QueryResponse(BaseModel):
    answer: str
    sources: List[str]

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    # 1. æ£€ç´¢
    docs = vectorstore.similarity_search(request.question, k=5)
    
    # 2. æ„å»º Prompt
    context = "\n".join([d.page_content for d in docs])
    prompt = PROMPT_TEMPLATE.format(context=context, question=request.question)
    
    # 3. ç”Ÿæˆç­”æ¡ˆ
    response = llm.invoke(prompt)
    
    # 4. è¿”å›
    return QueryResponse(
        answer=response.content,
        sources=[d.metadata.get("source", "") for d in docs]
    )
```

### 8.3 é…ç½®ç®¡ç†

```python
# config.py
import os

class Config:
    # å‘é‡æ•°æ®åº“
    CHROMADB_DIRECTORY = os.getenv("CHROMADB_DIR", "./chromaDB")
    CHROMADB_COLLECTION = os.getenv("CHROMADB_COLLECTION", "demo001")
    
    # æ¨¡å‹é…ç½®
    LLM_TYPE = os.getenv("LLM_TYPE", "oneapi")  # openai æˆ– oneapi
    LLM_MODEL = os.getenv("LLM_MODEL", "qwen-plus")
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-v1")
    
    # API é…ç½®
    ONEAPI_BASE = "http://139.224.72.218:3000/v1"
    ONEAPI_KEY = os.getenv("ONEAPI_KEY", "sk-...")
    
    # Prompt
    PROMPT_TEMPLATE = "prompt_template.txt"
```

---

## ä¹ã€æ€§èƒ½ä¼˜åŒ–

### 9.1 æ£€ç´¢ä¼˜åŒ–

```python
# 1. ç´¢å¼•ä¼˜åŒ–
# æ‰¹é‡æ·»åŠ ï¼Œæé«˜æ•ˆç‡
vectorstore.add_texts(documents, batch_size=100)

# 2. ç¼“å­˜ä¼˜åŒ–
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_embedding(text: str):
    return embeddings.embed_query(text)

# 3. å¼‚æ­¥ä¼˜åŒ–
import asyncio

async def async_retrieve(queries: list[str]):
    tasks = [vectorstore.asimilarity_search(q, k=3) for q in queries]
    results = await asyncio.gather(*tasks)
    return results
```

### 9.2 æˆæœ¬ä¼˜åŒ–

| ä¼˜åŒ–ç‚¹ | æ–¹æ³• | æˆæœ¬é™ä½ |
|--------|------|----------|
| Embedding æ¨¡å‹ | ä½¿ç”¨ text-embedding-3-small | ~50% |
| LLM æ¨¡å‹ | é—®é¢˜ç®€å•ç”¨ 4o-mini | ~80% |
| æ£€ç´¢æ•°é‡ | ç²¾ç¡®æ§åˆ¶ k å€¼ | ~30% |
| ç¼“å­˜ | çƒ­é—¨æŸ¥è¯¢ç¼“å­˜ | ~60% |

---

## åã€å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| æ£€ç´¢ä¸å‡†ç¡® | Embedding æ¨¡å‹ä¸åŒ¹é… | ä½¿ç”¨åŒæº Embedding |
| ä¸Šä¸‹æ–‡æˆªæ–­ | chunk_size è¿‡å¤§ | å‡å° chunk_size |
| é‡å¤å†…å®¹ | æ£€ç´¢ç»“æœç›¸ä¼¼ | MMR å»é‡ |
| å›ç­”ä¸ç›¸å…³ | Prompt ä¸æ¸…æ™° | ä¼˜åŒ– Prompt æ¨¡æ¿ |
| æ€§èƒ½å·® | åŒæ­¥é˜»å¡ | å¼‚æ­¥ + å¹¶å‘ |

---

## åä¸€ã€ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•

```python
PRODUCTION_CHECKLIST = {
    "æ•°æ®è´¨é‡": [
        "âœ… æ–‡æ¡£é¢„å¤„ç†å®Œæˆ",
        "âœ… chunk å¤§å°åˆç†",
        "âœ… å…ƒæ•°æ®å®Œæ•´"
    ],
    "ç´¢å¼•ä¼˜åŒ–": [
        "âœ… ç´¢å¼•æ„å»ºå®Œæˆ",
        "âœ… ç´¢å¼•å¤§å°ç›‘æ§",
        "âœ… å®šæœŸæ›´æ–°ç´¢å¼•"
    ],
    "æ£€ç´¢æ•ˆæœ": [
        "âœ… æ£€ç´¢ç›¸å…³æ€§å¥½",
        "âœ… å“åº”æ—¶é—´ < 1s",
        "âœ… é‡æ’åºç”Ÿæ•ˆ"
    ],
    "æœåŠ¡ç¨³å®š": [
        "âœ… API å¯ç”¨æ€§ > 99%",
        "âœ… é”™è¯¯é‡è¯•æœºåˆ¶",
        "âœ… æ—¥å¿—è®°å½•å®Œæ•´"
    ]
}
```

---

## åäºŒã€å­¦ä¹ èµ„æº

### æ–‡æ¡£
- LangChain RAG: https://python.langchain.com/docs/tutorials/rag/
- Chroma: https://docs.trychroma.com/
- RAG æœ€ä½³å®è·µ: https://github.com/langchain-ai/rag-evaluation

### é¡¹ç›®
- RagLangChainTest: https://github.com/NanGePlus/RagLangChainTest
- LangGraphChatBot: https://github.com/NanGePlus/LangGraphChatBot

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´: 2026-01-31*
*æ¥æº: NanGePlus RAG å­¦ä¹ *
